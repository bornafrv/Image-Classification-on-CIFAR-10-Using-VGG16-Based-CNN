# ðŸ§  Convolutional Neural Networks with VGG16 â€“ CIFAR-10 Image Classification

This project explores the power of **Convolutional Neural Networks (CNNs)** in image classification using the popular **CIFAR-10 dataset**. By leveraging **transfer learning** with the **VGG16 architecture**, we classify 60,000 color images (32Ã—32 pixels) into 10 distinct categories.

## ðŸŽ¯ Objectives

- Load and preprocess the CIFAR-10 dataset effectively.
- Fine-tune a **pretrained VGG16** model for CIFAR-10.
- Train and validate a robust CNN classifier.
- Visualize and evaluate the modelâ€™s performance.
- Analyze the contributions of convolutional vs. dense layers.

## ðŸ“¦ Dataset

- **CIFAR-10**: 60,000 images (50,000 training + 10,000 testing)
- 10 categories: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck

## ðŸ§ª Preprocessing

- Pixel normalization (scaling to [0,1])
- Resize images to match **VGG16 input** shape (224x224)
- One-hot encoding of class labels

## ðŸ§± Model Architecture

- Base: **VGG16 pretrained on ImageNet**
- **Convolutional layers** optionally frozen to preserve learned features
- Custom **Fully Connected (FC)** layers added for CIFAR-10 classification

## ðŸ‹ï¸â€â™€ï¸ Training Configuration

- **Optimizer**: Adam
- **Loss Function**: Categorical Crossentropy
- **Batch Size**: 32
- **Epochs**: 20+ (tunable)
- Real-time monitoring of accuracy and loss

## ðŸ“Š Evaluation

- Accuracy & Loss curves across epochs
- Confusion Matrix on test set
- Insightful discussion of **feature extraction vs classification**

## ðŸ”¬ Key Concepts

- CNNs extract hierarchical features: **edges â†’ textures â†’ shapes**
- FC layers serve as classifiers built on extracted features
- **Transfer learning** drastically boosts performance by reusing pretrained knowledge

---

> Course: *Artificial Intelligence*
> Instructor: Dr. YaghoobZadeh
> University of Tehran | School of Electrical & Computer Engineering